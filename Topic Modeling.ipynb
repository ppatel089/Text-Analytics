{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"coronavirus_tweets_cleaned_Latest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>location</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>...</th>\n",
       "      <th>cleaned_location_1</th>\n",
       "      <th>created_at_datetime</th>\n",
       "      <th>created_at_date</th>\n",
       "      <th>tokens_location</th>\n",
       "      <th>tokens_location_bigram</th>\n",
       "      <th>token_string</th>\n",
       "      <th>test_positive_y_n</th>\n",
       "      <th>cleaned_location_state</th>\n",
       "      <th>cleaned_location_country</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-03-21 17:34:31</td>\n",
       "      <td>29842</td>\n",
       "      <td>62327</td>\n",
       "      <td>980</td>\n",
       "      <td>At the same time the president is in the brief...</td>\n",
       "      <td>ðŸ‡ºðŸ‡¸ðŸ‡ºðŸ‡¸ðŸ‡ºðŸ‡¸</td>\n",
       "      <td>336</td>\n",
       "      <td>LizRNC</td>\n",
       "      <td>8476</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-21 17:34:31</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>[u'blank']</td>\n",
       "      <td>[]</td>\n",
       "      <td>time,presid,brief,room,talk,hope,drug,treat,wu...</td>\n",
       "      <td>True</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>time president room new hope drug treat report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-21 17:01:32</td>\n",
       "      <td>29842</td>\n",
       "      <td>62327</td>\n",
       "      <td>980</td>\n",
       "      <td>This is really gross, but we shouldn't be surp...</td>\n",
       "      <td>ðŸ‡ºðŸ‡¸ðŸ‡ºðŸ‡¸ðŸ‡ºðŸ‡¸</td>\n",
       "      <td>453</td>\n",
       "      <td>LizRNC</td>\n",
       "      <td>8476</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-21 17:01:32</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>[u'blank']</td>\n",
       "      <td>[]</td>\n",
       "      <td>realli,gross,shouldnt,surpriseddemocrat,push,c...</td>\n",
       "      <td>False</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>gross democrat conspiracy theory year barbara ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-03-21 03:11:06</td>\n",
       "      <td>69</td>\n",
       "      <td>12466</td>\n",
       "      <td>340</td>\n",
       "      <td>China hid information about the Wuhan Virus.\\r...</td>\n",
       "      <td>Green Cove Springs, FL</td>\n",
       "      <td>498</td>\n",
       "      <td>JudsonSapp</td>\n",
       "      <td>529</td>\n",
       "      <td>...</td>\n",
       "      <td>green cove springs</td>\n",
       "      <td>2020-03-21 03:11:06</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>[u'green', u'cove', u'spring', u'fl']</td>\n",
       "      <td>[(u'green', u'cove'), (u'cove', u'spring'), (u...</td>\n",
       "      <td>china,inform,wuhan,china,allow,wuhan,spread,pr...</td>\n",
       "      <td>False</td>\n",
       "      <td>Florida</td>\n",
       "      <td>US</td>\n",
       "      <td>china hid information virus china wuhan virus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-03-21 21:25:49</td>\n",
       "      <td>231</td>\n",
       "      <td>187501</td>\n",
       "      <td>755</td>\n",
       "      <td>This Coronavirus is the first thing theyâ€™re no...</td>\n",
       "      <td>New York</td>\n",
       "      <td>370</td>\n",
       "      <td>MarkSimoneNY</td>\n",
       "      <td>21075</td>\n",
       "      <td>...</td>\n",
       "      <td>york</td>\n",
       "      <td>2020-03-21 21:25:49</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>[u'new', u'york']</td>\n",
       "      <td>[(u'new', u'york')]</td>\n",
       "      <td>coronaviru,first,thing,proud,stamp,made,china,...</td>\n",
       "      <td>False</td>\n",
       "      <td>New York</td>\n",
       "      <td>US</td>\n",
       "      <td>coronavirus first thing theyâ€™re proud stamp ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-03-21 01:02:45</td>\n",
       "      <td>29842</td>\n",
       "      <td>62327</td>\n",
       "      <td>980</td>\n",
       "      <td>The people screaming about \"UKRAINE!\" througho...</td>\n",
       "      <td>ðŸ‡ºðŸ‡¸ðŸ‡ºðŸ‡¸ðŸ‡ºðŸ‡¸</td>\n",
       "      <td>243</td>\n",
       "      <td>LizRNC</td>\n",
       "      <td>8476</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-21 01:02:45</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>[u'blank']</td>\n",
       "      <td>[]</td>\n",
       "      <td>peopl,scream,ukrain,throughout,tell,ahead,curv...</td>\n",
       "      <td>False</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>people ukraine jan way #wuhanvirus fall fake n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           created_at  favourites_count  followers_count  \\\n",
       "0           0  2020-03-21 17:34:31             29842            62327   \n",
       "1           1  2020-03-21 17:01:32             29842            62327   \n",
       "2           2  2020-03-21 03:11:06                69            12466   \n",
       "3           3  2020-03-21 21:25:49               231           187501   \n",
       "4           4  2020-03-21 01:02:45             29842            62327   \n",
       "\n",
       "   friends_count                                          full_text  \\\n",
       "0            980  At the same time the president is in the brief...   \n",
       "1            980  This is really gross, but we shouldn't be surp...   \n",
       "2            340  China hid information about the Wuhan Virus.\\r...   \n",
       "3            755  This Coronavirus is the first thing theyâ€™re no...   \n",
       "4            980  The people screaming about \"UKRAINE!\" througho...   \n",
       "\n",
       "                 location  retweet_count   screen_name  statuses_count  ...  \\\n",
       "0                  ðŸ‡ºðŸ‡¸ðŸ‡ºðŸ‡¸ðŸ‡ºðŸ‡¸            336        LizRNC            8476  ...   \n",
       "1                  ðŸ‡ºðŸ‡¸ðŸ‡ºðŸ‡¸ðŸ‡ºðŸ‡¸            453        LizRNC            8476  ...   \n",
       "2  Green Cove Springs, FL            498    JudsonSapp             529  ...   \n",
       "3                New York            370  MarkSimoneNY           21075  ...   \n",
       "4                  ðŸ‡ºðŸ‡¸ðŸ‡ºðŸ‡¸ðŸ‡ºðŸ‡¸            243        LizRNC            8476  ...   \n",
       "\n",
       "   cleaned_location_1  created_at_datetime created_at_date  \\\n",
       "0                 NaN  2020-03-21 17:34:31      2020-03-21   \n",
       "1                 NaN  2020-03-21 17:01:32      2020-03-21   \n",
       "2  green cove springs  2020-03-21 03:11:06      2020-03-21   \n",
       "3                york  2020-03-21 21:25:49      2020-03-21   \n",
       "4                 NaN  2020-03-21 01:02:45      2020-03-21   \n",
       "\n",
       "                         tokens_location  \\\n",
       "0                             [u'blank']   \n",
       "1                             [u'blank']   \n",
       "2  [u'green', u'cove', u'spring', u'fl']   \n",
       "3                      [u'new', u'york']   \n",
       "4                             [u'blank']   \n",
       "\n",
       "                              tokens_location_bigram  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [(u'green', u'cove'), (u'cove', u'spring'), (u...   \n",
       "3                                [(u'new', u'york')]   \n",
       "4                                                 []   \n",
       "\n",
       "                                        token_string test_positive_y_n  \\\n",
       "0  time,presid,brief,room,talk,hope,drug,treat,wu...              True   \n",
       "1  realli,gross,shouldnt,surpriseddemocrat,push,c...             False   \n",
       "2  china,inform,wuhan,china,allow,wuhan,spread,pr...             False   \n",
       "3  coronaviru,first,thing,proud,stamp,made,china,...             False   \n",
       "4  peopl,scream,ukrain,throughout,tell,ahead,curv...             False   \n",
       "\n",
       "  cleaned_location_state cleaned_location_country  \\\n",
       "0                Unknown                  Unknown   \n",
       "1                Unknown                  Unknown   \n",
       "2                Florida                       US   \n",
       "3               New York                       US   \n",
       "4                Unknown                  Unknown   \n",
       "\n",
       "                                          clean_text  \n",
       "0  time president room new hope drug treat report...  \n",
       "1  gross democrat conspiracy theory year barbara ...  \n",
       "2  china hid information virus china wuhan virus ...  \n",
       "3  coronavirus first thing theyâ€™re proud stamp ch...  \n",
       "4  people ukraine jan way #wuhanvirus fall fake n...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The people screaming about \"UKRAINE!\" throughout Jan. are telling us they were way ahead of the curve on #WuhanVirus ðŸ™„\\r\\n\\r\\nLeave it to WaPo to fall back on its Fake News playbook -- unnamed hacks in the intel community making baseless claims https://t.co/ooj0wq4gXb'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['full_text'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: DeprecationWarning: invalid escape sequence \\(\n",
      "<>:11: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:11: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:39: DeprecationWarning: invalid escape sequence \\/\n",
      "<>:11: DeprecationWarning: invalid escape sequence \\(\n",
      "<>:11: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:11: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:39: DeprecationWarning: invalid escape sequence \\/\n",
      "<>:11: DeprecationWarning: invalid escape sequence \\(\n",
      "<>:11: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:11: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:39: DeprecationWarning: invalid escape sequence \\/\n",
      "<ipython-input-319-5d9e96093339>:11: DeprecationWarning: invalid escape sequence \\(\n",
      "  re_list = ['[\\()\\{}\\[]\\|\\]','http\\S*','www\\S*']\n",
      "<ipython-input-319-5d9e96093339>:11: DeprecationWarning: invalid escape sequence \\S\n",
      "  re_list = ['[\\()\\{}\\[]\\|\\]','http\\S*','www\\S*']\n",
      "<ipython-input-319-5d9e96093339>:11: DeprecationWarning: invalid escape sequence \\S\n",
      "  re_list = ['[\\()\\{}\\[]\\|\\]','http\\S*','www\\S*']\n",
      "<ipython-input-319-5d9e96093339>:39: DeprecationWarning: invalid escape sequence \\/\n",
      "  cleantext= re.sub('[\\/.,!?*%\\-_\\â€¦]',' ',cleantext)\n"
     ]
    }
   ],
   "source": [
    "# Method to clean text\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "STOPWORDS = STOPWORDS + ['com','urls','scammer','null','zqzpru','covid','coronavirus','#coronavirus','#covid19','&amp']\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "import re\n",
    "re_list = ['[\\()\\{}\\[]\\|\\]','http\\S*','www\\S*']\n",
    "\n",
    "generic_re = re.compile( '|'.join(re_list) )\n",
    "cleaner = re.compile(generic_re)\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "emoji_cleaner= re.compile(emoji_pattern)\n",
    "\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    To clean the text summary\n",
    "    It uses various regex expressions, stopwords removal and Lemmetization of words \n",
    "    '''\n",
    "    if not text:\n",
    "        print('None')\n",
    "    else:\n",
    "        cleantext = text.lower().strip()\n",
    "        cleantext = re.sub('[\\\\n,\\\\n-]',' ', cleantext)\n",
    "        cleantext = cleaner.sub(' ',cleantext)\n",
    "        cleantext= emoji_cleaner.sub(' ', cleantext)\n",
    "        cleantext= re.sub('[\\/.,!?*%\\-_\\â€¦]',' ',cleantext)\n",
    "        cleantext= re.sub('\\'',' ',cleantext)\n",
    "        cleantext= re.sub(r'\"',' ',cleantext)\n",
    "        cleantext =[word for word in cleantext.split() if word not in STOPWORDS and (2 < len(word) < 15)]\n",
    "        cleantext= [lemmatizer.lemmatize(w) for w in cleantext]\n",
    "        words_pos = nltk.pos_tag(cleantext)\n",
    "        tweet_noun_adj= [word for (word, pos) in words_pos if (pos in ['NN','NNS','JJ'])] \n",
    "        \n",
    "        return ' '.join(tweet_noun_adj)\n",
    "  \n",
    "apply_clean = lambda x: clean_text(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text']= data.full_text.apply(lambda x: apply_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'people ukraine jan way #wuhanvirus fall fake news playbook unnamed hack intel community baseless claim'"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[]\n",
    "for sentence in df['clean_text']:\n",
    "    word_tokens=[word for word in sentence.split()]\n",
    "    tokens.append(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "my_dict = Dictionary(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = [my_dict.doc2bow(doc) for doc in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "tfidf_vectorizer = TfidfModel(dtm) \n",
    "tfidf = tfidf_vectorizer[dtm] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lda_DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "lda_DTM = models.LdaModel(corpus=dtm, num_topics=4, id2word=my_dict, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"virus\" + 0.011*\"world\" + 0.009*\"day\" + 0.008*\"trump\" + 0.008*\"india\" + 0.008*\"#wuhanvirus\" + 0.007*\"&amp;\" + 0.007*\"chinese\" + 0.007*\"good\" + 0.007*\"new\"'),\n",
       " (1,\n",
       "  '0.017*\"patient\" + 0.015*\"test\" + 0.014*\"positive\" + 0.011*\"azithromycin\" + 0.008*\"doctor\" + 0.007*\"treatment\" + 0.007*\"state\" + 0.007*\"today\" + 0.006*\"hospital\" + 0.006*\"#wuhanvirus\"'),\n",
       " (2,\n",
       "  '0.017*\"people\" + 0.013*\"time\" + 0.012*\"health\" + 0.009*\"new\" + 0.007*\"help\" + 0.007*\"pandemic\" + 0.006*\"public\" + 0.006*\"#sarscov2\" + 0.006*\"mask\" + 0.006*\"home\"'),\n",
       " (3,\n",
       "  '0.023*\"death\" + 0.021*\"case\" + 0.020*\"#stayhome\" + 0.012*\"number\" + 0.010*\"virus\" + 0.009*\"people\" + 0.008*\"new\" + 0.008*\"corona\" + 0.008*\"time\" + 0.007*\"state\"')]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_DTM.print_topics()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Topic0: Caution\n",
    "Topic1: Lock Down- StayHome\n",
    "Topic2: Cases Tested Positive\n",
    "Topic3: Medicines  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lda_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "lda_tfidf = models.LdaModel(corpus=tfidf, num_topics=4, id2word=my_dict, passes=20,alpha=0.2,eta=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"azithromycin\" + 0.003*\"social\" + 0.003*\"study\" + 0.003*\"patient\" + 0.003*\"distancing\" + 0.002*\"life\" + 0.002*\"donâ€™t\" + 0.002*\"#coronaupdate\" + 0.002*\"virus\" + 0.002*\"&amp;\"'),\n",
       " (1,\n",
       "  '0.005*\"death\" + 0.004*\"virus\" + 0.004*\"test\" + 0.003*\"positive\" + 0.003*\"people\" + 0.003*\"day\" + 0.003*\"country\" + 0.003*\"number\" + 0.003*\"rate\" + 0.002*\"case\"'),\n",
       " (2,\n",
       "  '0.005*\"#stayhome\" + 0.005*\"case\" + 0.004*\"home\" + 0.004*\"new\" + 0.003*\"stay\" + 0.003*\"patient\" + 0.003*\"#stayhealthy\" + 0.003*\"positive\" + 0.002*\"ventilator\" + 0.002*\"#covid\"'),\n",
       " (3,\n",
       "  '0.003*\"world\" + 0.003*\"time\" + 0.003*\"#breakthechain\" + 0.003*\"today\" + 0.002*\"india\" + 0.002*\"people\" + 0.002*\"mask\" + 0.002*\"worker\" + 0.002*\"#wuhanvirus\" + 0.002*\"china\"')]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tfidf.print_topics()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Topic0: Death Metrics\n",
    "Topic1: Cases Tested Positive\n",
    "Topic2: Lock Down- StayHome\n",
    "Topic3: Caution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pyLDAvis\\nimport pyLDAvis.gensim\\npyLDAvis.enable_notebook()\\nvis = pyLDAvis.gensim.prepare(lda_tfidf, tfidf, my_dict)\\nvis'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_tfidf, tfidf, my_dict)\n",
    "vis'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
